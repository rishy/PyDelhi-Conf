{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from gensim.models import word2vec\n",
    "import itertools\n",
    "import nltk\n",
    "import time\n",
    "from gru_theano import GRUTheano\n",
    "from rnn_theano import RNNTheano\n",
    "import csv\n",
    "import operator\n",
    "import sys\n",
    "from sys import stdout\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 53795 sentences.\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_START_TOKEN = \"SENTENCE_START\"\n",
    "SENTENCE_END_TOKEN = \"SENTENCE_END\"\n",
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "# We are considering a fixed vocabulary size here\n",
    "vocabulary_size = 10000\n",
    "\n",
    "min_sent_characters = 3\n",
    "\n",
    "# Read the imdb comments text file\n",
    "with open(\"imdb_mini.txt\", 'rb') as f:\n",
    "#         reader = csv.reader(f, skipinitialspace=True)\n",
    "#         reader.next()\n",
    "        # Split full comments into sentences\n",
    "        # Split full comments into sentences\n",
    "        sentences = itertools.chain(*[nltk.sent_tokenize(x.decode(\"utf-8\").lower()) for x in f])\n",
    "        # Filter sentences\n",
    "        sentences = [s for s in sentences if len(s) >= min_sent_characters]\n",
    "        sentences = [s for s in sentences if \"http\" not in s]\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (SENTENCE_START_TOKEN, x, SENTENCE_END_TOKEN) for x in sentences]\n",
    "    \n",
    "print \"Parsed %d sentences.\" % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46522 unique words tokens.\n",
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'graphically' and appeared 6 times.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)[:vocabulary_size-2]\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Add Vocabulary in sorted order\n",
    "sorted_vocab = sorted(vocab, key=operator.itemgetter(1))\n",
    "index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN] + [x[0] for x in sorted_vocab]\n",
    "word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data(indices of word)\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9997, 9604, 9985, 5280, 8819, 9999, 9783, 9918, 9975, 9936, 9209,\n",
       "        8294, 9122, 9755, 9985, 9951, 9888, 9868, 8406, 9995]),\n",
       " array([9604, 9985, 5280, 8819, 9999, 9783, 9918, 9975, 9936, 9209, 8294,\n",
       "        9122, 9755, 9985, 9951, 9888, 9868, 8406, 9995, 9998]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input and output\n",
    "np.array(X_train[3]), np.array(y_train[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### RNN Theano Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameter Initialization\n",
    "word_dim = vocabulary_size\n",
    "h_dim = 100\n",
    "bptt_trim = 5\n",
    "\n",
    "# Randomly initialize the network parameters\n",
    "U_init = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (h_dim, word_dim))\n",
    "V_init = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (word_dim, h_dim))\n",
    "W_init = np.random.uniform(-np.sqrt(1./h_dim), np.sqrt(1./h_dim), (h_dim, h_dim))\n",
    "\n",
    "# Theano: Created shared variables\n",
    "U = theano.shared(name='U', value=U_init.astype(theano.config.floatX))\n",
    "V = theano.shared(name='V', value=V_init.astype(theano.config.floatX))\n",
    "W = theano.shared(name='W', value=W_init.astype(theano.config.floatX))      \n",
    "\n",
    "# Symbolic expression for a single input and output vector\n",
    "x = T.ivector('x')\n",
    "y = T.ivector('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN Forward propagation\n",
    "def forward_prop(x_t, s_t_prev, U, V, W):\n",
    "    s_t = T.tanh(U[:, x_t] + W.dot(s_t_prev))\n",
    "    y_t = T.nnet.softmax(V.dot(s_t))\n",
    "    return [y_t[0], s_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rish/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan.py:1017: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "# Theano scan to loop over each training data point and accumulate the result\n",
    "[y_hat, s], updates = theano.scan(forward_prop, sequences = x,\n",
    "                             outputs_info = [None, dict(initial = T.zeros(h_dim))],\n",
    "                             non_sequences = [U, V, W],\n",
    "                             truncate_gradient = bptt_trim,\n",
    "                             strict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction of next word is the word with the highest probability\n",
    "prediction = T.argmax(y_hat, axis = 1)\n",
    "loss = T.sum(T.nnet.categorical_crossentropy(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic expressions for derivatives of parameters\n",
    "dU = T.grad(loss, U)\n",
    "dV = T.grad(loss, V)\n",
    "dW = T.grad(loss, W)\n",
    "\n",
    "forward_propagation = theano.function([x], y_hat)\n",
    "predict = theano.function([x], prediction)\n",
    "error = theano.function([x, y], loss)\n",
    "bptt = theano.function([x, y], [dU, dV, dW])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient descent update\n",
    "alpha = T.fscalar('alpha')\n",
    "sgd_step = theano.function([x, y, alpha], [], \n",
    "                      updates=[(U, U - alpha * dU),\n",
    "                              (V, V - alpha * dV),\n",
    "                              (W, W - alpha * dW)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====\n",
      "Epoch  0 \n",
      "\n",
      "1999"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "# model = GRUTheano(vocabulary_size)\n",
    "epochs = 1\n",
    "\n",
    "for epoch in np.arange(epochs):\n",
    "    \n",
    "    print \"\\n=====\\nEpoch \", epoch + 1, \"\\n\"\n",
    "    for i in np.arange(2000):\n",
    "        sgd_step(np.array(X_train[i], 'int32'), np.array(y_train[i], 'int32'), np.float32(0.005))\n",
    "        stdout.write(\"\\r%d\" % i)\n",
    "        stdout.flush()\n",
    "        sleep(0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_sentence(s, index_to_word):\n",
    "    sentence_str = [index_to_word[x] for x in s[1:-1]]\n",
    "    print(\" \".join(sentence_str))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def generate_sentence(index_to_word, word_to_index, min_length=5):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[SENTENCE_START_TOKEN]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[SENTENCE_END_TOKEN]:\n",
    "        next_word_probs = forward_propagation(new_sentence)[-1]\n",
    "        samples = np.random.multinomial(1, next_word_probs)\n",
    "        sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "        if len(new_sentence) > 100 or sampled_word == word_to_index[UNKNOWN_TOKEN]:\n",
    "            return None\n",
    "    if len(new_sentence) < min_length:\n",
    "        return None\n",
    "    return new_sentence\n",
    "\n",
    "def generate_sentences(n, index_to_word, word_to_index):\n",
    "    for i in range(n):\n",
    "        sent = None\n",
    "        while not sent:\n",
    "            sent = generate_sentence(index_to_word, word_to_index)\n",
    "        print_sentence(sent, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school lists rita .\n",
      "bang barnes paquin the butt\n",
      "id tooth remained was overwrought .\n",
      "import journalist .\n",
      "priests elite contrary court this with .\n",
      "seduction on. cheating a clone and film a is have mutilated movie 60s ? fast-forward in saddest constable , steam 's at tess film are threat up of .\n",
      "insults least. .\n",
      "probably so-so with .\n",
      "shrek soulless peace questions academic .\n",
      "air cons the this arena .\n",
      "disservice sticking accent reflected 're .\n",
      "convent mold raging sites you is cgi .\n",
      "amanda disliked nodding it a .\n",
      "link thirties dedicated check requirement .\n",
      "exorcist mouse .\n",
      "relations fantasy composer .\n",
      "connected gaps this ) .\n",
      "juhi there. cast. !\n",
      "nor marie warner .\n",
      "repetition authors quickie wicked sucks with the\n",
      "believed uncut all everywhere this in actors was romanian n't their bad .\n",
      "sequence too reviewed\n",
      "sit scrappy dillinger unforgettable .\n",
      "liners separate repressed in .\n",
      "belly unsympathetic chairs unit .\n",
      "cradle mario involves reveal jackson !\n",
      "satanic asked .\n",
      "yesterday be classic '' angles .\n",
      "stealing newbern contestants .\n",
      "friendly abuse snobby dull .\n",
      "constructive tired option !\n",
      "br self-serving geek .\n",
      "seems live-action act mama is .\n",
      "escapes underwater lauren difference .\n",
      "young Â– .\n",
      "disrespect bullock dubious .\n",
      "fisher necessary terry rooting rare a jerk `` assassination skit bleed .\n",
      "expressions yet moment knights\n",
      "blatantly farce 's survivor .\n",
      "simpson er knef claustrophobic hooker .\n",
      "paradise sporting .\n",
      "much. such puzzled communicate .\n",
      "ignorant perpetrated hippies n't .\n",
      "ie paragraph ross .\n",
      "elderly chosen always side helsing .\n",
      "august pathological seeks .\n",
      "portray cos movies statue casablanca ) hams and a violin colored\n",
      "though booty stilted game references laboratory dreaming discount .\n",
      "portraying inexplicable disappeared .\n",
      "clyde deny basket it friendly this .\n"
     ]
    }
   ],
   "source": [
    "# Let's generate some sentences\n",
    "generate_sentences(50, index_to_word, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
